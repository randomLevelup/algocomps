# ParkerGPT: AI-Powered Melody Jazzifier

ParkerGPT processes (monophonic) MIDI melodies using:

1. A Markov chain model for generating jazz chord progressions based on the input melody
2. A bidirectional neural network model that inserts licks, turns, and other jazz goodies into the melody at controlled intervals

The system outputs a MIDI file containing both the jazzified melody and accompanying jazz chord progressions.


## Table of Contents
- [Requirements](#requirements)
- [Installation](#installation)
- [Usage](#usage)
  - [Training Mode](#training-mode)
  - [Inference Mode](#inference-mode)
- [How It Works](#how-it-works)
  - [Chord Generation](#chord-generation)
  - [Melodic Jazzification](#melodic-jazzification)

## Requirements
- Python 3.7+
- PyTorch
- music21
- Additional dependencies listed in requirements.txt

## Installation

To install the dependencies, run the following:
```bash
pip install -r requirements.txt
```

## Usage

ParkerGPT operates in two modes: training mode and inference mode.

### Training Mode

Training mode is used to train the neural network models on a corpus of music data:

```bash
python comp2.py --train_model --train_data_dir [DIR] --model_save_dir [DIR]
```
Some notes about training:

- The preprocessing step will store a `data.pkl` file containing the preprocessed corpus in your current working directory. This is useful for tuning the model, as preprocessing takes a good amount of time.
- Training mode will generate *two* models (one forward and one backward) and save them both to `model_save_dir`. This is necessary because the jazzified segments are generated in both a forward and a backward direction simultaneously (more on this in [how it works](#melodic-jazzification))
- You may also specify custom training hyperparameters with the argument `--model_config /path/to/config.yaml`. The default hyperparameters are listed in `default_config.yaml`

### Inference Mode

Inference mode splices an input melody with jazzy improvisations generated by the trained model:

```bash
python comp2.py --input_path input_melody.mid --output_path output_jazz.mid
```

You may also include the following flags for further refinement of the jazzification process:

- `--chord_gen_depth D`, where `D` is an integer. This determines the number of Markov generation cycles to run during the chord generation step. The default is 5 cycles.
- `--context_size S`, where `S` is an integer. This determines the size of the context window that the program will use to make inferences about the current melody segment. The context size is measured in quarter notes. The default size is 3.

## How It Works

### Chord Generation
The chord generation process builds and uses a *state-based* Markov table to make substitutions for each chord. The process is as follows:

1. First, simple triads are derived from the melody.
2. Next, the Markov table is instantiated. This table maps *states* (groups of 2 chords) to the next chord in a sequence. Since this table has a mapping for every possible 2-chord state, it's domain is quite large. So the table isn't stored, rather generated using the `generate_markov_table` function.
3. Finally, the substitution process is run, starting with the simple triads, as many times as specified by the `--chord_gen_depth` argument.

### Melodic Jazzification:
The melodic jazzification and splicing process is a bit complicated, so here's some annotated sheet music with the first 4 bars of a jazzified version of "Final Countdown" by Europe:

**Melodic Transformation**:

- Bidirectional neural network models (forward and backward) generate improvised segments.
- The models use contextual information from the original melody
- Segments of the original melody are preserved and interspersed with generated content

**Output**: The system combines the generated melody and chord accompaniment into a single MIDI file.


##

*Created by Jupiter Westbard*
